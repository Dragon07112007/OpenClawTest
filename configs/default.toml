[model]
vocab_size = 32000
n_layers = 6
n_heads = 8
d_model = 512
d_ff = 2048

[training]
batch_size = 16
seq_length = 256
learning_rate = 0.0003
epochs = 3
seed = 42
save_every_epochs = 1
precision = "off"
grad_accum_steps = 1
dataloader_workers = 0
dataloader_prefetch_factor = 2
dataloader_pin_memory = false
log_every_steps = 10

[data]
dataset_name = "wikitext-2"
dataset_path = "data/wikitext-2"

[device]
preference = "auto"
strict = false
