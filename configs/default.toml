[model]
vocab_size = 32000
n_layers = 6
n_heads = 8
d_model = 512
d_ff = 2048

[training]
batch_size = 16
seq_length = 256
learning_rate = 0.0003
epochs = 3

[data]
dataset_name = "wikitext-2"
dataset_path = "data/wikitext-2"

[device]
preference = "auto"
